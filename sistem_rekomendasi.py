# -*- coding: utf-8 -*-
"""sistem rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HiMiFO1qJdVCcSR5y-LdpegC8sgNy1Ib

# Sistem Rekomendasi

Menuliskan library
"""

import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics.pairwise import cosine_similarity

"""## Loading Dataset

Loading Dataset
"""

movies = pd.read_csv('movies.csv')
rating = pd.read_csv('ratings.csv')

display(movies.head())
print('\n')
display(rating.head())

"""## Univariate EDA

Menampilkan Informasi umum dan mengecek missing values data.
"""

# Menampilkan informasi umum dari kedua dataset
print("Informasi dataset Movies:")
print(movies.info())

print("\nInformasi dataset Ratings:")
print(rating.info())

# Cek missing values
print("\nCek missing values:")
print("Movies:\n", movies.isnull().sum())
print("Ratings:\n", rating.isnull().sum())

"""Menampilkan distribusi rating."""

plt.figure(figsize=(8, 5))
sns.histplot(rating['rating'], bins=10, kde=True)
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.grid(True)
plt.show()

"""Menampilkan jumlah rating per-film."""

# Hitung jumlah rating untuk setiap movie
rating_count = rating['movieId'].value_counts().head(20)

# Gabungkan dengan judul film
top_movies = movies[movies['movieId'].isin(rating_count.index)]

# Visualisasi film dengan rating terbanyak
plt.figure(figsize=(12, 6))
sns.barplot(x='title', y='movieId', data=top_movies.set_index('movieId').loc[rating_count.index].reset_index(), palette='viridis')
plt.xticks(rotation=90)
plt.title('20 Film dengan Jumlah Rating Terbanyak')
plt.xlabel('Judul Film')
plt.ylabel('Jumlah Rating')
plt.show()

"""Menampilkan Rata-rata rating per-film."""

# Menghitung rata-rata rating per movie
mean_ratings = rating.groupby('movieId')['rating'].mean()

# Gabungkan dengan jumlah rating
movie_stats = rating.groupby('movieId').agg({'rating': ['mean', 'count']})
movie_stats.columns = ['mean_rating', 'rating_count']
movie_stats = movie_stats.reset_index()

# Gabungkan dengan judul film
movie_stats = movie_stats.merge(movies, on='movieId')

# Tampilkan 10 film dengan rata-rata rating tertinggi (min 50 rating)
top_rated_movies = movie_stats[movie_stats['rating_count'] >= 50].sort_values(by='mean_rating', ascending=False).head(10)

# Visualisasi
plt.figure(figsize=(12, 6))
sns.barplot(x='title', y='mean_rating', data=top_rated_movies, palette='rocket')
plt.xticks(rotation=90)
plt.title('10 Film dengan Rata-rata Rating Tertinggi (â‰¥ 50 rating)')
plt.xlabel('Judul Film')
plt.ylabel('Rata-rata Rating')
plt.show()

"""Analisis genre: Menampilkan jumlah film per genre."""

# Pecah genre menjadi list
movies['genres'] = movies['genres'].apply(lambda x: x.split('|'))

# Hitung jumlah masing-masing genre
from collections import Counter
genre_counter = Counter([genre for genres in movies['genres'] for genre in genres])
genre_df = pd.DataFrame(genre_counter.items(), columns=['Genre', 'Jumlah Film']).sort_values(by='Jumlah Film', ascending=False)

# Visualisasi
plt.figure(figsize=(10, 6))
sns.barplot(x='Jumlah Film', y='Genre', data=genre_df, palette='cubehelix')
plt.title('Jumlah Film per Genre')
plt.xlabel('Jumlah Film')
plt.ylabel('Genre')
plt.show()

"""Mengubah timestamp ke format tanggal."""

# Konversi timestamp ke datetime
rating['datetime'] = pd.to_datetime(rating['timestamp'], unit='s')

# Buat kolom tahun rating
rating['year'] = rating['datetime'].dt.year

# Visualisasi jumlah rating per tahun
plt.figure(figsize=(10, 5))
sns.countplot(x='year', data=rating, palette='crest')
plt.xticks(rotation=45)
plt.title('Jumlah Rating per Tahun')
plt.xlabel('Tahun')
plt.ylabel('Jumlah Rating')
plt.show()

"""## Data Preprocessing

Hapus duplikasi data.
"""

movies.drop_duplicates(subset=['movieId', 'title'], inplace=True)
rating.drop_duplicates(inplace=True)

"""Pisahkan genre berdasarkan delimiter '|'"""

if isinstance(movies['genres'].iloc[0], str):
    movies['genres'] = movies['genres'].apply(lambda x: x.split('|'))

"""Gunakan MultiLabelBinarizer untuk one-hot encoding kolom genre"""

mlb = MultiLabelBinarizer()
genre_matrix = pd.DataFrame(mlb.fit_transform(movies['genres']), columns=mlb.classes_)

"""Gabungkan dengan kolom movieId dan title"""

movies = pd.concat([movies[['movieId', 'title']], genre_matrix], axis=1)

"""Konversi kolom timestamp menjadi format datetime"""

rating['datetime'] = pd.to_datetime(rating['timestamp'], unit='s')

"""Tambahkan kolom tahun dari timestamp"""

rating['rating_year'] = rating['datetime'].dt.year

"""Gabungkan dataset rating dan movies berdasarkan movieId"""

movie_rating = pd.merge(rating, movies, on='movieId', how='inner')

# Tampilkan jumlah data akhir
print("Jumlah data setelah preprocessing:")
print("Movies:", len(movies))
print("Ratings:", len(rating))
print("Gabungan Movie-Rating:", len(movie_rating))

# Tampilkan 5 baris pertama untuk verifikasi
movie_rating.head()

"""Menampilkan informasi data yang sudah digabung."""

movie_rating.info()

"""## Content Based Filering

Import library yang diperlukan.
"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import random
import warnings
warnings.filterwarnings('ignore')

""" Persiapan Data Genre dan Matriks Cosine Similarity"""

# Asumsikan movie_rating sudah termasuk kolom movieId, title, genre one-hot (Action, Comedy, dsb.)
genre_columns = movie_rating.columns[9:]  # Mulai dari kolom 'Action' hingga 'Western'
movie_features = movie_rating[['movieId', 'title'] + list(genre_columns)].drop_duplicates('movieId')

# Matriks fitur genre
genre_matrix = movie_features[genre_columns].values

# Matriks cosine similarity antar film
cosine_sim = cosine_similarity(genre_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_features['movieId'], columns=movie_features['movieId'])

"""Fungsi Rekomendasi Film"""

def movie_recommendations(movie_id, k=5):
    if movie_id not in cosine_sim_df:
        return pd.DataFrame()

    sim_scores = cosine_sim_df[movie_id].drop(movie_id, errors='ignore')
    top_k_ids = sim_scores.nlargest(k).index
    return movie_features[movie_features['movieId'].isin(top_k_ids)][['movieId', 'title']]

"""Data Rating dan User Likes

"""

# Ambil data rating
rating_data = movie_rating[['userId', 'movieId', 'rating']]

# Asumsikan rating >= 4 dianggap "disukai"
user_likes = rating_data[rating_data['rating'] >= 4].groupby('userId')['movieId'].apply(list)

# Buat mapping movieId <-> title
id_to_title = movie_features.set_index('movieId')['title'].to_dict()
title_to_id = movie_features.set_index('title')['movieId'].to_dict()

"""Contoh Penggunaan"""

# Cek movieId dari "Toy Story (1995)"
title_query = "Toy Story (1995)"
movie_id_query = title_to_id[title_query]

# Tampilkan rekomendasi film mirip
recommendations = movie_recommendations(movie_id_query, k=5)
print("Rekomendasi untuk:", title_query)
display(recommendations)

"""Evaluasi Precision@K"""

def evaluate_precision_at_k(user_id, k=5):
    liked_ids = user_likes.get(user_id, [])
    if not liked_ids:
        return None

    query_id = liked_ids[0]  # ambil satu movie yang disukai sebagai query
    recs = movie_recommendations(query_id, k=k)
    if recs.empty:
        return None

    recommended_ids = recs['movieId'].tolist()
    hits = len(set(recommended_ids) & set(liked_ids))
    return hits / k

"""Evaluasi ke Banyak User"""

random.seed(42)
sample_users = random.sample(list(user_likes.index), 100)
scores = [evaluate_precision_at_k(uid) for uid in sample_users]
scores = [s for s in scores if s is not None]

avg_precision_at_k = sum(scores) / len(scores)
print(f"Average Precision@5: {avg_precision_at_k:.4f}")

"""## Collaborative Filtering

Import library
"""

import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

"""Mapping user dan movie ID ke angka integer"""

df = rating[['userId', 'movieId', 'rating']].dropna()

# Mapping user dan movie ke angka
user_ids = df['userId'].unique().tolist()
movie_ids = df['movieId'].unique().tolist()

user_to_encoded = {x: i for i, x in enumerate(user_ids)}
movie_to_encoded = {x: i for i, x in enumerate(movie_ids)}

df['user'] = df['userId'].map(user_to_encoded)
df['movie'] = df['movieId'].map(movie_to_encoded)

# Normalisasi rating ke rentang 0-1
df['rating'] = df['rating'].astype('float32')
min_rating, max_rating = df['rating'].min(), df['rating'].max()
df['norm_rating'] = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

# Acak dan split data
df = df.sample(frac=1, random_state=42)
x = df[['user', 'movie']].values
y = df['norm_rating'].values

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""Bangun model collaborative filtering"""

class RecommenderNet(Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=regularizers.l2(1e-4))
        self.movie_embedding = layers.Embedding(num_movies, embedding_size,
                                                embeddings_initializer='he_normal',
                                                embeddings_regularizer=regularizers.l2(1e-4))
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_bias = layers.Embedding(num_movies, 1)

        self.dense1 = layers.Dense(64, activation='relu')
        self.dropout = layers.Dropout(0.7)
        self.output_layer = layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        user_vec = self.user_embedding(inputs[:, 0])
        movie_vec = self.movie_embedding(inputs[:, 1])
        user_bias = self.user_bias(inputs[:, 0])
        movie_bias = self.movie_bias(inputs[:, 1])

        dot = tf.reduce_sum(user_vec * movie_vec, axis=1, keepdims=True)
        x = dot + user_bias + movie_bias
        x = self.dense1(x)
        x = self.dropout(x)
        return self.output_layer(x)

"""Latih model"""

num_users = len(user_to_encoded)
num_movies = len(movie_to_encoded)

model = RecommenderNet(num_users, num_movies, embedding_size=50)
model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=128,
    epochs=30,
    callbacks=[early_stop, lr_reduce],
    verbose=1
)

"""Visualisasi RMSE"""

plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Val RMSE')
plt.legend()
plt.title('Training vs Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.grid(True)
plt.show()

"""Rekomendasi Film untuk User"""

encoded_to_movie = {v: k for k, v in movie_to_encoded.items()}

user_id = df.userId.sample(1).iloc[0]
movies_watched = rating[rating['userId'] == user_id]['movieId'].tolist()

movies_not_watched = list(
    set(movie_ids) - set(movies_watched)
)
movies_not_watched_encoded = [movie_to_encoded[m] for m in movies_not_watched if m in movie_to_encoded]

user_encoded = user_to_encoded[user_id]
user_movie_array = np.hstack(
    ([[user_encoded]] * len(movies_not_watched_encoded), np.array(movies_not_watched_encoded).reshape(-1, 1))
)

ratings = model.predict(user_movie_array).flatten()
top_k_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [encoded_to_movie[movies_not_watched_encoded[i]] for i in top_k_indices]

# Tampilkan rekomendasi
print(f"Rekomendasi untuk user {user_id}")
print("-" * 30)
recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(f"{row.title}")